#map = affine_map<(d0) -> (d0)>
module {
  func.func @var_mean_welford_kernel(%arg0: memref<?xi8>, %arg1: memref<?xi8>, %arg2: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 0 : i32}, %arg3: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg4: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32) attributes {SyncBlockLockArgIdx = 0 : i64, WorkspaceArgIdx = 1 : i64, global_kernel = "local", mix_mode = "aiv"} {
    %c64 = arith.constant 64 : index
    %c4 = arith.constant 4 : index
    %c4_i32 = arith.constant 4 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i32 = arith.constant 0 : i32
    %c64_i32 = arith.constant 64 : i32
    %cst_0 = arith.constant 1.000000e+00 : f32
    %0 = tensor.empty() : tensor<4xf32>
    %1 = linalg.fill ins(%cst_0 : f32) outs(%0 : tensor<4xf32>) -> tensor<4xf32>
    %2 = tensor.empty() : tensor<4x64xf32>
    %3 = linalg.fill ins(%cst_0 : f32) outs(%2 : tensor<4x64xf32>) -> tensor<4x64xf32>
    %4 = linalg.fill ins(%cst : f32) outs(%2 : tensor<4x64xf32>) -> tensor<4x64xf32>
    %5 = arith.muli %arg11, %c4_i32 : i32
    %6 = tensor.empty() : tensor<4xi32>
    %7 = linalg.generic {indexing_maps = [#map], iterator_types = ["parallel"]} outs(%6 : tensor<4xi32>) {
    ^bb0(%out: i32):
      %39 = linalg.index 0 : index
      %40 = arith.index_cast %39 : index to i32
      linalg.yield %40 : i32
    } -> tensor<4xi32>
    %expanded = tensor.expand_shape %7 [[0, 1]] output_shape [4, 1] : tensor<4xi32> into tensor<4x1xi32>
    %8 = tensor.empty() : tensor<4x1xi32>
    %9 = linalg.fill ins(%5 : i32) outs(%8 : tensor<4x1xi32>) -> tensor<4x1xi32>
    %10 = arith.addi %9, %expanded : tensor<4x1xi32>
    %11 = arith.index_cast %5 : i32 to index
    %12 = arith.index_cast %arg6 : i32 to index
    %13 = arith.muli %11, %12 : index
    %reinterpret_cast = memref.reinterpret_cast %arg3 to offset: [%11], sizes: [4, 1], strides: [1, 1] : memref<?xf32> to memref<4x1xf32, strided<[1, 1], offset: ?>>
    %reinterpret_cast_1 = memref.reinterpret_cast %arg4 to offset: [%11], sizes: [4, 1], strides: [1, 1] : memref<?xf32> to memref<4x1xf32, strided<[1, 1], offset: ?>>
    %14 = linalg.fill ins(%arg5 : i32) outs(%8 : tensor<4x1xi32>) -> tensor<4x1xi32>
    %15 = arith.cmpi slt, %10, %14 : tensor<4x1xi32>
    %16 = tensor.empty() : tensor<64xi32>
    %17 = linalg.generic {indexing_maps = [#map], iterator_types = ["parallel"]} outs(%16 : tensor<64xi32>) {
    ^bb0(%out: i32):
      %39 = linalg.index 0 : index
      %40 = arith.index_cast %39 : index to i32
      linalg.yield %40 : i32
    } -> tensor<64xi32>
    %expanded_2 = tensor.expand_shape %17 [[0, 1]] output_shape [1, 64] : tensor<64xi32> into tensor<1x64xi32>
    %18 = tensor.empty() : tensor<1x64xi32>
    %19 = linalg.fill ins(%arg6 : i32) outs(%18 : tensor<1x64xi32>) -> tensor<1x64xi32>
    %20 = tensor.empty() : tensor<4x64xi1>
    %collapsed = tensor.collapse_shape %15 [[0, 1]] : tensor<4x1xi1> into tensor<4xi1>
    %broadcasted = linalg.broadcast ins(%collapsed : tensor<4xi1>) outs(%20 : tensor<4x64xi1>) dimensions = [1] 
    %21:3 = scf.for %arg14 = %c0_i32 to %arg6 step %c64_i32 iter_args(%arg15 = %4, %arg16 = %4, %arg17 = %4) -> (tensor<4x64xf32>, tensor<4x64xf32>, tensor<4x64xf32>)  : i32 {
      %39 = linalg.fill ins(%arg14 : i32) outs(%18 : tensor<1x64xi32>) -> tensor<1x64xi32>
      %40 = arith.addi %39, %expanded_2 : tensor<1x64xi32>
      %41 = arith.cmpi slt, %40, %19 : tensor<1x64xi32>
      %collapsed_10 = tensor.collapse_shape %41 [[0, 1]] : tensor<1x64xi1> into tensor<64xi1>
      %broadcasted_11 = linalg.broadcast ins(%collapsed_10 : tensor<64xi1>) outs(%20 : tensor<4x64xi1>) dimensions = [0] 
      %42 = arith.andi %broadcasted, %broadcasted_11 : tensor<4x64xi1>
      %43 = arith.index_cast %arg14 : i32 to index
      %44 = arith.addi %13, %43 : index
      %reinterpret_cast_12 = memref.reinterpret_cast %arg2 to offset: [%44], sizes: [4, 64], strides: [%12, 1] : memref<?xf32> to memref<4x64xf32, strided<[?, 1], offset: ?>>
      %alloc = memref.alloc() : memref<4x64xf32>
      %45 = arith.addi %11, %c4 : index
      %46 = arith.index_cast %arg5 : i32 to index
      %47 = arith.maxsi %11, %46 : index
      %48 = arith.minsi %45, %47 : index
      %49 = arith.subi %48, %11 : index
      %50 = arith.addi %43, %c64 : index
      %51 = arith.maxsi %43, %12 : index
      %52 = arith.minsi %50, %51 : index
      %53 = arith.subi %52, %43 : index
      %54 = arith.minsi %49, %c4 : index
      %55 = arith.minsi %53, %c64 : index
      %56 = arith.cmpi slt, %54, %c4 : index
      %57 = arith.cmpi slt, %55, %c64 : index
      %58 = arith.ori %56, %57 : i1
      scf.if %58 {
        linalg.fill ins(%cst : f32) outs(%alloc : memref<4x64xf32>)
      }
      %subview_13 = memref.subview %reinterpret_cast_12[0, 0] [%54, %55] [1, 1] : memref<4x64xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[?, 1], offset: ?>>
      %subview_14 = memref.subview %alloc[0, 0] [%54, %55] [1, 1] : memref<4x64xf32> to memref<?x?xf32, strided<[64, 1]>>
      memref.copy %subview_13, %subview_14 : memref<?x?xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[64, 1]>>
      %59 = bufferization.to_tensor %alloc restrict writable : memref<4x64xf32>
      %60 = arith.uitofp %42 : tensor<4x64xi1> to tensor<4x64xf32>
      %61 = arith.addf %arg17, %60 : tensor<4x64xf32>
      %62 = arith.maxnumf %61, %3 : tensor<4x64xf32>
      %63 = arith.mulf %arg16, %arg17 : tensor<4x64xf32>
      %64 = arith.addf %63, %59 : tensor<4x64xf32>
      %65 = arith.divf %64, %62 : tensor<4x64xf32>
      %66 = arith.subf %59, %65 : tensor<4x64xf32>
      %67 = arith.subf %59, %arg16 : tensor<4x64xf32>
      %68 = arith.mulf %66, %67 : tensor<4x64xf32>
      %69 = arith.mulf %68, %60 : tensor<4x64xf32>
      %70 = arith.addf %arg15, %69 : tensor<4x64xf32>
      scf.yield %70, %65, %61 : tensor<4x64xf32>, tensor<4x64xf32>, tensor<4x64xf32>
    }
    %22 = linalg.fill ins(%cst : f32) outs(%0 : tensor<4xf32>) -> tensor<4xf32>
    %reduced = linalg.reduce ins(%21#2 : tensor<4x64xf32>) outs(%22 : tensor<4xf32>) dimensions = [1] 
      (%in: f32, %init: f32) {
        %39 = arith.addf %in, %init : f32
        linalg.yield %39 : f32
      }
    %23 = arith.mulf %21#1, %21#2 : tensor<4x64xf32>
    %reduced_3 = linalg.reduce ins(%23 : tensor<4x64xf32>) outs(%22 : tensor<4xf32>) dimensions = [1] 
      (%in: f32, %init: f32) {
        %39 = arith.addf %in, %init : f32
        linalg.yield %39 : f32
      }
    %24 = arith.maxnumf %reduced, %1 : tensor<4xf32>
    %25 = arith.divf %reduced_3, %24 : tensor<4xf32>
    %expanded_4 = tensor.expand_shape %25 [[0, 1]] output_shape [4, 1] : tensor<4xf32> into tensor<4x1xf32>
    %broadcasted_5 = linalg.broadcast ins(%25 : tensor<4xf32>) outs(%2 : tensor<4x64xf32>) dimensions = [1] 
    %26 = arith.subf %21#1, %broadcasted_5 : tensor<4x64xf32>
    %27 = arith.mulf %21#2, %26 : tensor<4x64xf32>
    %28 = arith.mulf %27, %26 : tensor<4x64xf32>
    %29 = arith.addf %21#0, %28 : tensor<4x64xf32>
    %reduced_6 = linalg.reduce ins(%29 : tensor<4x64xf32>) outs(%22 : tensor<4xf32>) dimensions = [1] 
      (%in: f32, %init: f32) {
        %39 = arith.addf %in, %init : f32
        linalg.yield %39 : f32
      }
    %30 = arith.subi %arg6, %arg7 : i32
    %31 = arith.sitofp %30 : i32 to f32
    %32 = linalg.fill ins(%31 : f32) outs(%0 : tensor<4xf32>) -> tensor<4xf32>
    %33 = arith.divf %reduced_6, %32 : tensor<4xf32>
    %expanded_7 = tensor.expand_shape %33 [[0, 1]] output_shape [4, 1] : tensor<4xf32> into tensor<4x1xf32>
    %34 = arith.addi %11, %c4 : index
    %35 = arith.index_cast %arg5 : i32 to index
    %36 = arith.maxsi %11, %35 : index
    %37 = arith.minsi %34, %36 : index
    %38 = arith.subi %37, %11 : index
    %extracted_slice = tensor.extract_slice %expanded_4[0, 0] [%38, 1] [1, 1] : tensor<4x1xf32> to tensor<?x1xf32>
    %subview = memref.subview %reinterpret_cast_1[0, 0] [%38, 1] [1, 1] : memref<4x1xf32, strided<[1, 1], offset: ?>> to memref<?x1xf32, strided<[1, 1], offset: ?>>
    bufferization.materialize_in_destination %extracted_slice in writable %subview : (tensor<?x1xf32>, memref<?x1xf32, strided<[1, 1], offset: ?>>) -> ()
    %extracted_slice_8 = tensor.extract_slice %expanded_7[0, 0] [%38, 1] [1, 1] : tensor<4x1xf32> to tensor<?x1xf32>
    %subview_9 = memref.subview %reinterpret_cast[0, 0] [%38, 1] [1, 1] : memref<4x1xf32, strided<[1, 1], offset: ?>> to memref<?x1xf32, strided<[1, 1], offset: ?>>
    bufferization.materialize_in_destination %extracted_slice_8 in writable %subview_9 : (tensor<?x1xf32>, memref<?x1xf32, strided<[1, 1], offset: ?>>) -> ()
    return
  }
}

