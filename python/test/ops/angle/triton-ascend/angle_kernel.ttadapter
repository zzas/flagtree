module {
  func.func private @__hmf_atanf(f32) -> f32 attributes {llvm.readnone}
  func.func @angle_kernel(%arg0: memref<?xi8>, %arg1: memref<?xi8>, %arg2: memref<?xf16> {tt.divisibility = 16 : i32, tt.tensor_kind = 0 : i32}, %arg3: memref<?xf16> {tt.tensor_kind = 0 : i32}, %arg4: memref<?xf16> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32) attributes {SyncBlockLockArgIdx = 0 : i64, WorkspaceArgIdx = 1 : i64, global_kernel = "local", mix_mode = "aiv"} {
    %cst = arith.constant 0.000000e+00 : f16
    %c1024 = arith.constant 1024 : index
    %c1024_i32 = arith.constant 1024 : i32
    %cst_0 = arith.constant -3.14159274 : f32
    %cst_1 = arith.constant 3.14159274 : f32
    %cst_2 = arith.constant -1.57079637 : f32
    %cst_3 = arith.constant 1.57079637 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1024xf32>
    %1 = linalg.fill ins(%cst_4 : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32>
    %2 = linalg.fill ins(%cst_3 : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32>
    %3 = linalg.fill ins(%cst_2 : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32>
    %4 = linalg.fill ins(%cst_1 : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32>
    %5 = linalg.fill ins(%cst_0 : f32) outs(%0 : tensor<1024xf32>) -> tensor<1024xf32>
    %6 = arith.muli %arg9, %c1024_i32 : i32
    %7 = arith.index_cast %6 : i32 to index
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%7], sizes: [1024], strides: [1] : memref<?xf16> to memref<1024xf16, strided<[1], offset: ?>>
    %alloc = memref.alloc() : memref<1024xf16>
    %8 = arith.addi %7, %c1024 : index
    %9 = arith.index_cast %arg5 : i32 to index
    %10 = arith.maxsi %7, %9 : index
    %11 = arith.minsi %8, %10 : index
    %12 = arith.subi %11, %7 : index
    %13 = arith.cmpi slt, %12, %c1024 : index
    scf.if %13 {
      linalg.fill ins(%cst : f16) outs(%alloc : memref<1024xf16>)
    }
    %subview = memref.subview %reinterpret_cast[0] [%12] [1] : memref<1024xf16, strided<[1], offset: ?>> to memref<?xf16, strided<[1], offset: ?>>
    %subview_5 = memref.subview %alloc[0] [%12] [1] : memref<1024xf16> to memref<?xf16, strided<[1]>>
    memref.copy %subview, %subview_5 : memref<?xf16, strided<[1], offset: ?>> to memref<?xf16, strided<[1]>>
    %14 = bufferization.to_tensor %alloc restrict writable : memref<1024xf16>
    %reinterpret_cast_6 = memref.reinterpret_cast %arg3 to offset: [%7], sizes: [1024], strides: [1] : memref<?xf16> to memref<1024xf16, strided<[1], offset: ?>>
    %alloc_7 = memref.alloc() : memref<1024xf16>
    scf.if %13 {
      linalg.fill ins(%cst : f16) outs(%alloc_7 : memref<1024xf16>)
    }
    %subview_8 = memref.subview %reinterpret_cast_6[0] [%12] [1] : memref<1024xf16, strided<[1], offset: ?>> to memref<?xf16, strided<[1], offset: ?>>
    %subview_9 = memref.subview %alloc_7[0] [%12] [1] : memref<1024xf16> to memref<?xf16, strided<[1]>>
    memref.copy %subview_8, %subview_9 : memref<?xf16, strided<[1], offset: ?>> to memref<?xf16, strided<[1]>>
    %15 = bufferization.to_tensor %alloc_7 restrict writable : memref<1024xf16>
    %16 = arith.extf %14 : tensor<1024xf16> to tensor<1024xf32>
    %17 = arith.extf %15 : tensor<1024xf16> to tensor<1024xf32>
    %18 = arith.cmpf oeq, %16, %1 : tensor<1024xf32>
    %19 = arith.divf %17, %16 : tensor<1024xf32>
    %mapped = linalg.map { func.call {callee = @__hmf_atanf} } ins(%19 : tensor<1024xf32>) outs(%19 : tensor<1024xf32>)
    %20 = arith.select %18, %1, %mapped : tensor<1024xi1>, tensor<1024xf32>
    %21 = arith.cmpf ogt, %17, %1 : tensor<1024xf32>
    %22 = arith.andi %18, %21 : tensor<1024xi1>
    %23 = arith.select %22, %2, %20 : tensor<1024xi1>, tensor<1024xf32>
    %24 = arith.cmpf olt, %17, %1 : tensor<1024xf32>
    %25 = arith.andi %18, %24 : tensor<1024xi1>
    %26 = arith.select %25, %3, %23 : tensor<1024xi1>, tensor<1024xf32>
    %27 = arith.cmpf olt, %16, %1 : tensor<1024xf32>
    %28 = arith.cmpf oge, %17, %1 : tensor<1024xf32>
    %29 = arith.andi %27, %28 : tensor<1024xi1>
    %30 = arith.select %29, %4, %1 : tensor<1024xi1>, tensor<1024xf32>
    %31 = arith.andi %27, %24 : tensor<1024xi1>
    %32 = arith.select %31, %5, %1 : tensor<1024xi1>, tensor<1024xf32>
    %33 = arith.addf %26, %30 : tensor<1024xf32>
    %34 = arith.addf %33, %32 : tensor<1024xf32>
    %reinterpret_cast_10 = memref.reinterpret_cast %arg4 to offset: [%7], sizes: [1024], strides: [1] : memref<?xf16> to memref<1024xf16, strided<[1], offset: ?>>
    %35 = arith.truncf %34 : tensor<1024xf32> to tensor<1024xf16>
    %extracted_slice = tensor.extract_slice %35[0] [%12] [1] : tensor<1024xf16> to tensor<?xf16>
    %subview_11 = memref.subview %reinterpret_cast_10[0] [%12] [1] : memref<1024xf16, strided<[1], offset: ?>> to memref<?xf16, strided<[1], offset: ?>>
    bufferization.materialize_in_destination %extracted_slice in writable %subview_11 : (tensor<?xf16>, memref<?xf16, strided<[1], offset: ?>>) -> ()
    return
  }
}

