#map = affine_map<(d0) -> (d0)>
module {
  func.func @min_kernel(%arg0: memref<?xi8>, %arg1: memref<?xi8>, %arg2: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 0 : i32}, %arg3: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg4: memref<?xi64> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: i32) attributes {SyncBlockLockArgIdx = 0 : i64, WorkspaceArgIdx = 1 : i64, global_kernel = "local", mix_mode = "aiv"} {
    %c-1_i32 = arith.constant -1 : i32
    %c256 = arith.constant 256 : index
    %c8 = arith.constant 8 : index
    %c8_i32 = arith.constant 8 : i32
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    %cst = arith.constant 0x7F800000 : f32
    %0 = tensor.empty() : tensor<8xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<8xf32>) -> tensor<8xf32>
    %2 = tensor.empty() : tensor<8xi64>
    %3 = linalg.fill ins(%c0_i64 : i64) outs(%2 : tensor<8xi64>) -> tensor<8xi64>
    %4 = arith.muli %arg11, %c8_i32 : i32
    %5 = tensor.empty() : tensor<256xi32>
    %6 = linalg.generic {indexing_maps = [#map], iterator_types = ["parallel"]} outs(%5 : tensor<256xi32>) {
    ^bb0(%out: i32):
      %19 = linalg.index 0 : index
      %20 = arith.index_cast %19 : index to i32
      linalg.yield %20 : i32
    } -> tensor<256xi32>
    %7 = tensor.empty() : tensor<8x256xi32>
    %broadcasted = linalg.broadcast ins(%6 : tensor<256xi32>) outs(%7 : tensor<8x256xi32>) dimensions = [0] 
    %8:2 = scf.for %arg14 = %c0_i32 to %arg6 step %c256_i32 iter_args(%arg15 = %1, %arg16 = %3) -> (tensor<8xf32>, tensor<8xi64>)  : i32 {
      %19 = arith.index_cast %4 : i32 to index
      %20 = arith.index_cast %arg6 : i32 to index
      %21 = arith.muli %19, %20 : index
      %22 = arith.index_cast %arg7 : i32 to index
      %23 = arith.muli %21, %22 : index
      %24 = arith.muli %20, %22 : index
      %25 = arith.index_cast %arg14 : i32 to index
      %26 = arith.muli %25, %22 : index
      %27 = arith.index_cast %arg12 : i32 to index
      %28 = arith.addi %23, %27 : index
      %29 = arith.addi %28, %26 : index
      %reinterpret_cast_3 = memref.reinterpret_cast %arg2 to offset: [%29], sizes: [8, 256], strides: [%24, %22] : memref<?xf32> to memref<8x256xf32, strided<[?, ?], offset: ?>>
      %alloc = memref.alloc() : memref<8x256xf32>
      %30 = arith.addi %19, %c8 : index
      %31 = arith.index_cast %arg5 : i32 to index
      %32 = arith.maxsi %19, %31 : index
      %33 = arith.minsi %30, %32 : index
      %34 = arith.subi %33, %19 : index
      %35 = arith.addi %25, %c256 : index
      %36 = arith.maxsi %25, %20 : index
      %37 = arith.minsi %35, %36 : index
      %38 = arith.subi %37, %25 : index
      %39 = arith.minsi %34, %c8 : index
      %40 = arith.minsi %38, %c256 : index
      %41 = arith.cmpi slt, %39, %c8 : index
      %42 = arith.cmpi slt, %40, %c256 : index
      %43 = arith.ori %41, %42 : i1
      scf.if %43 {
        linalg.fill ins(%cst : f32) outs(%alloc : memref<8x256xf32>)
      }
      %subview_4 = memref.subview %reinterpret_cast_3[0, 0] [%39, %40] [1, 1] : memref<8x256xf32, strided<[?, ?], offset: ?>> to memref<?x?xf32, strided<[?, ?], offset: ?>>
      %subview_5 = memref.subview %alloc[0, 0] [%39, %40] [1, 1] : memref<8x256xf32> to memref<?x?xf32, strided<[256, 1]>>
      memref.copy %subview_4, %subview_5 : memref<?x?xf32, strided<[?, ?], offset: ?>> to memref<?x?xf32, strided<[256, 1]>>
      %44 = bufferization.to_tensor %alloc restrict writable : memref<8x256xf32>
      %45 = tensor.empty() : tensor<8xi32>
      %46 = linalg.fill ins(%c-1_i32 : i32) outs(%45 : tensor<8xi32>) -> tensor<8xi32>
      %reduced:2 = linalg.reduce ins(%44, %broadcasted : tensor<8x256xf32>, tensor<8x256xi32>) outs(%1, %46 : tensor<8xf32>, tensor<8xi32>) dimensions = [1]  {reduce_mode = "min_with_index"}
        (%in: f32, %in_6: i32, %init: f32, %init_7: i32) {
          %53 = arith.cmpf olt, %in, %init : f32
          %54 = arith.cmpf oeq, %in, %init : f32
          %55 = arith.cmpi slt, %in_6, %init_7 : i32
          %56 = arith.andi %54, %55 : i1
          %57 = arith.ori %53, %56 : i1
          %58 = arith.select %57, %in, %init : f32
          %59 = arith.select %57, %in_6, %init_7 : i32
          linalg.yield %58, %59 : f32, i32
        }
      %47 = arith.cmpf olt, %reduced#0, %arg15 : tensor<8xf32>
      %48 = arith.select %47, %reduced#0, %arg15 : tensor<8xi1>, tensor<8xf32>
      %49 = linalg.fill ins(%arg14 : i32) outs(%45 : tensor<8xi32>) -> tensor<8xi32>
      %50 = arith.addi %49, %reduced#1 : tensor<8xi32>
      %51 = arith.extsi %50 : tensor<8xi32> to tensor<8xi64>
      %52 = arith.select %47, %51, %arg16 : tensor<8xi1>, tensor<8xi64>
      scf.yield %48, %52 : tensor<8xf32>, tensor<8xi64>
    }
    %9 = arith.index_cast %4 : i32 to index
    %10 = arith.index_cast %arg7 : i32 to index
    %11 = arith.muli %9, %10 : index
    %12 = arith.index_cast %arg12 : i32 to index
    %13 = arith.addi %11, %12 : index
    %reinterpret_cast = memref.reinterpret_cast %arg3 to offset: [%13], sizes: [8], strides: [%10] : memref<?xf32> to memref<8xf32, strided<[?], offset: ?>>
    %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [%13], sizes: [8], strides: [%10] : memref<?xi64> to memref<8xi64, strided<[?], offset: ?>>
    %14 = arith.addi %9, %c8 : index
    %15 = arith.index_cast %arg5 : i32 to index
    %16 = arith.maxsi %9, %15 : index
    %17 = arith.minsi %14, %16 : index
    %18 = arith.subi %17, %9 : index
    %extracted_slice = tensor.extract_slice %8#0[0] [%18] [1] : tensor<8xf32> to tensor<?xf32>
    %subview = memref.subview %reinterpret_cast[0] [%18] [1] : memref<8xf32, strided<[?], offset: ?>> to memref<?xf32, strided<[?], offset: ?>>
    bufferization.materialize_in_destination %extracted_slice in writable %subview : (tensor<?xf32>, memref<?xf32, strided<[?], offset: ?>>) -> ()
    %extracted_slice_1 = tensor.extract_slice %8#1[0] [%18] [1] : tensor<8xi64> to tensor<?xi64>
    %subview_2 = memref.subview %reinterpret_cast_0[0] [%18] [1] : memref<8xi64, strided<[?], offset: ?>> to memref<?xi64, strided<[?], offset: ?>>
    bufferization.materialize_in_destination %extracted_slice_1 in writable %subview_2 : (tensor<?xi64>, memref<?xi64, strided<[?], offset: ?>>) -> ()
    return
  }
}

