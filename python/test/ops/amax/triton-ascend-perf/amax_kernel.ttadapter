module {
  func.func @amax_kernel(%arg0: memref<?xi8>, %arg1: memref<?xi8>, %arg2: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 0 : i32}, %arg3: memref<?xf32> {tt.divisibility = 16 : i32, tt.tensor_kind = 1 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32) attributes {SyncBlockLockArgIdx = 0 : i64, WorkspaceArgIdx = 1 : i64, global_kernel = "local", mix_mode = "aiv"} {
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c8 = arith.constant 8 : index
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256_i32 = arith.constant 256 : i32
    %cst = arith.constant 0xFF800000 : f32
    %0 = tensor.empty() : tensor<8x256xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<8x256xf32>) -> tensor<8x256xf32>
    %2 = arith.divsi %arg5, %arg5 : i32
    scf.for %arg11 = %c0_i32 to %2 step %c1_i32  : i32 {
      %3 = arith.muli %arg11, %arg5 : i32
      %4 = arith.addi %arg8, %3 : i32
      %5 = arith.muli %4, %c8_i32 : i32
      %6 = arith.index_cast %5 : i32 to index
      %7 = arith.index_cast %arg4 : i32 to index
      %8 = arith.muli %6, %7 : index
      %reinterpret_cast = memref.reinterpret_cast %arg3 to offset: [%6], sizes: [8, 1], strides: [1, 1] : memref<?xf32> to memref<8x1xf32, strided<[1, 1], offset: ?>>
      %9 = scf.for %arg12 = %c0_i32 to %arg4 step %c256_i32 iter_args(%arg13 = %1) -> (tensor<8x256xf32>)  : i32 {
        %16 = arith.index_cast %arg12 : i32 to index
        %17 = arith.addi %8, %16 : index
        %reinterpret_cast_0 = memref.reinterpret_cast %arg2 to offset: [%17], sizes: [8, 256], strides: [%7, 1] : memref<?xf32> to memref<8x256xf32, strided<[?, 1], offset: ?>>
        %alloc = memref.alloc() : memref<8x256xf32>
        %18 = arith.addi %6, %c8 : index
        %19 = arith.maxsi %6, %c1 : index
        %20 = arith.minsi %18, %19 : index
        %21 = arith.subi %20, %6 : index
        %22 = arith.addi %16, %c256 : index
        %23 = arith.maxsi %16, %7 : index
        %24 = arith.minsi %22, %23 : index
        %25 = arith.subi %24, %16 : index
        %26 = arith.minsi %21, %c8 : index
        %27 = arith.minsi %25, %c256 : index
        %28 = arith.cmpi slt, %26, %c8 : index
        %29 = arith.cmpi slt, %27, %c256 : index
        %30 = arith.ori %28, %29 : i1
        scf.if %30 {
          linalg.fill ins(%cst : f32) outs(%alloc : memref<8x256xf32>)
        }
        %subview_1 = memref.subview %reinterpret_cast_0[0, 0] [%26, %27] [1, 1] : memref<8x256xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[?, 1], offset: ?>>
        %subview_2 = memref.subview %alloc[0, 0] [%26, %27] [1, 1] : memref<8x256xf32> to memref<?x?xf32, strided<[256, 1]>>
        memref.copy %subview_1, %subview_2 : memref<?x?xf32, strided<[?, 1], offset: ?>> to memref<?x?xf32, strided<[256, 1]>>
        %31 = bufferization.to_tensor %alloc restrict writable : memref<8x256xf32>
        %32 = arith.maxnumf %arg13, %31 : tensor<8x256xf32>
        scf.yield %32 : tensor<8x256xf32>
      }
      %10 = tensor.empty() : tensor<8xf32>
      %11 = linalg.fill ins(%cst : f32) outs(%10 : tensor<8xf32>) -> tensor<8xf32>
      %reduced = linalg.reduce ins(%9 : tensor<8x256xf32>) outs(%11 : tensor<8xf32>) dimensions = [1] 
        (%in: f32, %init: f32) {
          %16 = arith.maxnumf %in, %init : f32
          linalg.yield %16 : f32
        }
      %expanded = tensor.expand_shape %reduced [[0, 1]] output_shape [8, 1] : tensor<8xf32> into tensor<8x1xf32>
      %12 = arith.addi %6, %c8 : index
      %13 = arith.maxsi %6, %c1 : index
      %14 = arith.minsi %12, %13 : index
      %15 = arith.subi %14, %6 : index
      %extracted_slice = tensor.extract_slice %expanded[0, 0] [%15, 1] [1, 1] : tensor<8x1xf32> to tensor<?x1xf32>
      %subview = memref.subview %reinterpret_cast[0, 0] [%15, 1] [1, 1] : memref<8x1xf32, strided<[1, 1], offset: ?>> to memref<?x1xf32, strided<[1, 1], offset: ?>>
      bufferization.materialize_in_destination %extracted_slice in writable %subview : (tensor<?x1xf32>, memref<?x1xf32, strided<[1, 1], offset: ?>>) -> ()
    }
    return
  }
}

