import math

import torch
import triton
import triton.language as tl

# active driver
driver = triton.runtime.driver.active
# torch.cuda, torch.aipu, torch.npu
torch_device_fn = triton.runtime.driver.active.get_device_interface()
# device
if hasattr(driver, "get_active_torch_device"):
    device = triton.runtime.driver.active.get_active_torch_device()
else:
    device = triton.runtime.driver.active.get_current_device()


@triton.jit
def get_dtype_max(dtype: tl.constexpr):
    """get a value which is greater that all other values of that dtype"""
    # extract the tl.dtype from tl.constexpr so as to use its methods
    dtype_ = dtype.value
    if dtype_.is_floating():
        value: tl.constexpr = float("inf")
        return value
    if dtype_.is_int_signed():
        width: tl.constexpr = dtype_.int_bitwidth
        value: tl.constexpr = 2 ** (width - 1) - 1
        return value
    if dtype_.is_int_unsigned():
        width: tl.constexpr = dtype_.int_bitwidth
        value: tl.constexpr = 2**width - 1
        return value


@triton.jit
def argmin_kernel(
    inp,
    out_index,
    M,
    N,
    K,
    BLOCK_M: tl.constexpr = 8,
    BLOCK_N: tl.constexpr = 16,
):
    # set offset
    pid_m = tl.program_id(0)
    # pid_k = tl.program_id(1)
    for pid_k in range(K):
        m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)

        dtype = inp.type.element_ty
        acc_type = tl.float32 if dtype is tl.bfloat16 else dtype
        max_value = get_dtype_max(dtype)
        min_values = tl.full([BLOCK_M], dtype=acc_type, value=max_value)
        argmin_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)
        for start_n in range(0, N, BLOCK_N):
            n_offset = start_n + tl.arange(0, BLOCK_N)
            offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k
            mask = m_offset[:, None] < M and n_offset[None, :] < N
            inp_ptrs = inp + offset
            inp_vals = tl.load(inp_ptrs, mask=mask, other=max_value)
            # tl.bfloat is promoted to tl.float32 by tl.min
            local_min, local_argmin = tl.min(
                inp_vals, 1, return_indices=True, return_indices_tie_break_left=True
            )
            # if return indices is not supported, call a tl.argmin in addition
            # local_argmin = tl.argmin(inp_vals, 1)
            update = local_min < min_values
            min_values = tl.where(update, local_min, min_values)
            argmin_values = tl.where(update, start_n + local_argmin, argmin_values)

        offset_index = m_offset * K + pid_k
        out_index_ptrs = out_index + offset_index
        mask1 = m_offset < M
        tl.store(out_index_ptrs, argmin_values, mask=mask1)


def argmin(inp, dim=None, keepdim=False, *, dtype=None):
    if dim is not None:
        assert dim >= -inp.ndim and dim < inp.ndim, "Invalid dim"
        shape = inp.shape
        dim = dim % inp.ndim
        N = shape[dim]
        M = math.prod(shape[:dim])
        K = inp.numel() // M // N

        inp = inp.contiguous()

        shape_list = list(shape)
        shape_list[dim] = 1
        out_index = torch.empty(shape_list, dtype=torch.int64, device=inp.device)
        if not keepdim:
            out_index = torch.squeeze(out_index, dim)

        grid = lambda meta: (
            triton.cdiv(M, meta["BLOCK_M"]),
            # K,
        )
        with torch_device_fn.device(inp.device):
            argmin_kernel[grid](
                inp,
                out_index,
                M,
                N,
                K,
            )

        return out_index


def check(name, ref, res, equal_nan=False, reduce_dim=1, atol=1e-4):
    RESOLUTION = {
        torch.bool: 0,
        torch.uint8: 0,
        torch.int8: 0,
        torch.int16: 0,
        torch.int32: 0,
        torch.int64: 0,
        torch.float8_e4m3fn: 1e-3,
        torch.float8_e5m2: 1e-3,
        torch.float8_e4m3fnuz: 1e-3,
        torch.float8_e5m2fnuz: 1e-3,
        torch.float16: 1e-3,
        torch.float32: 1.3e-6,
        torch.bfloat16: 0.016,
        torch.float64: 1e-7,
        torch.complex32: 1e-3,
        torch.complex64: 1.3e-6,
    }
    res = res.cpu()
    print(
        f"The maximum difference out {name} between torch and triton is "
        f"{torch.max(torch.abs(ref - res))}"
    )
    rtol = RESOLUTION[ref.dtype]
    assert torch.allclose(res, ref, atol=atol * reduce_dim, rtol=rtol), (res, ref)


if __name__ == "__main__":
    # param
    shape = (1, 32)
    dim = 1
    keepdim = True
    dtype = torch.float32

    # inp
    inp = torch.randn(shape, dtype=dtype, device=device)
    ref_inp = inp.cpu()

    # op
    ref_out = torch.argmin(ref_inp, dim=dim, keepdim=keepdim)
    res_out = argmin(inp, dim=dim, keepdim=keepdim)
    check("value", ref_out, res_out)
